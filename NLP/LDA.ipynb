{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc85eae0",
   "metadata": {},
   "source": [
    "=> **LDA** (Latent Dirichlet Allocation): é um modelo estatístico usado para descobrir tópicos ocultos em grandes coleções de textos. O LDA assume que cada documento em um conjunto de textos é composto por uma mistura de tópicos e que cada tópico é representado por um conjunto de palavras. Ele funciona assim:\n",
    "\n",
    " - Escolhe aleatoriamente um conjunto de tópicos para cada documento.\n",
    "\n",
    " - Observa as palavras no documento e ajusta a distribuição de tópicos com base em quais palavras aparecem juntas frequentemente.\n",
    "\n",
    " - Após várias iterações, o modelo aprende a associar palavras que pertencem ao mesmo tópico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d716a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "papers = pd.read_csv(\"papers.csv\")\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dd78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def text_cleaner(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    custom_stopwords = set(stopwords.words('english'))\n",
    "    cleaned_tokens = [token for token in tokens if token not in custom_stopwords]\n",
    "    cleaned_tokens = [token for token in cleaned_tokens if len(token) > 2]\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "papers['text_processed'] = papers['paper_text'].progress_apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10cde816",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'progress_apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_28812\\4070384100.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      7\u001b[39m     max_df=\u001b[32m0.95\u001b[39m,\n\u001b[32m      8\u001b[39m     max_features=\u001b[32m5000\u001b[39m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m papers[\u001b[33m'tfidf'\u001b[39m] = papers[\u001b[33m'text_processed'\u001b[39m].progress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: tfidf.transform([x]).toarray()[\u001b[32m0\u001b[39m])\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m tfidf.fit(papers[\u001b[33m'text_processed'\u001b[39m])\n",
      "\u001b[32mc:\\Users\\belch\\anaconda3\\envs\\llm311\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6295\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._accessors\n\u001b[32m   6296\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m self._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6297\u001b[39m         ):\n\u001b[32m   6298\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self[name]\n\u001b[32m-> \u001b[39m\u001b[32m6299\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m object.__getattribute__(self, name)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Series' object has no attribute 'progress_apply'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tfidf = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=stopwords.words('english'),\n",
    "    min_df=10,\n",
    "    max_df=0.95,\n",
    "    max_features=5000\n",
    ")\n",
    "\n",
    "papers['tfidf'] = papers['text_processed'].progress_apply(lambda x: tfidf.transform([x]).toarray()[0])\n",
    "\n",
    "\n",
    "tfidf.fit(papers['text_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c16b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers['tfidf'] = papers['text_processed'].progress_apply(lambda x: tfidf.transform([x]).toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a86afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document-Term Matrix\n",
    "dtm = pd.DataFrame(papers['tfidf'].to_list(),\n",
    "                   columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import numpy as np\n",
    "\n",
    "lda = LDA(n_components=5, random_state=42).fit(dtm.values)\n",
    "topics = lda.transform(dtm.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf413b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_topics(model, feature_names, n_top_words=5):\n",
    "    word_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[idx] for idx in top_features_ind]\n",
    "        word_dict[topic_idx] = top_features\n",
    "    return pd.DataFrame(word_dict)\n",
    "\n",
    "get_model_topics(lda, dtm.columns, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa91bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers['topic'] =  np.argmax(topics, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b103d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers.topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b305b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_all_wordclouds(papers, num_topics=5):\n",
    "    fig, axes = plt.subplots(1, num_topics, figsize=(20, 5))  # 1 linha, 5 colunas\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        texts = ' '.join(papers[papers['topic'] == i]['text_processed'])\n",
    "        wordcloud = WordCloud(width=800, height=800, background_color='white', min_font_size=10).generate(texts)\n",
    "\n",
    "        ax.imshow(wordcloud, interpolation='bilinear')\n",
    "        ax.axis('off')  # Remove os eixos\n",
    "        ax.set_title(f\"Tópico {i}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Chamada da função\n",
    "draw_all_wordclouds(papers)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
