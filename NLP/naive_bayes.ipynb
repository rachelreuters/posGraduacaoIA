{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d723666",
   "metadata": {},
   "source": [
    "=> **Embedding**: Embedding é uma técnica usada em aprendizado de máquina e processamento de linguagem natural (NLP) para representar dados, como palavras ou imagens, em um formato matemático que pode ser entendido por algoritmos.Eles transformam elementos de dados (como palavras ou imagens) em vetores numéricos dentro de um espaço dimensional, permitindo que máquinas processem informações de maneira eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef419bb",
   "metadata": {},
   "source": [
    "Temos que vetorizar numericamente os textos. Diferentes formas de preencher valores:  \n",
    "=> **One-Hot** : apenas considera se o documento tem ou nao o token.  \n",
    "=> **TF**: Apenas considera a frequencia do token  \n",
    "=> **TF-IDF**: Frequencia do token ponderada em relacao aos documentos .   Quantas vezes o token aparece no documento dividido por quantas vezes o token aparece em todos os documentos juntos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd682ca",
   "metadata": {},
   "source": [
    "=> **Teorema de Bayes**: O Teorema de Bayes é um método para atualizar probabilidades com base em novas informações. Ele responde perguntas do tipo: Dado que um evento ocorreu, qual é a chance de outra coisa ter acontecido antes?  \n",
    "P(A|B) = P(A)*P(B|A)/P(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729a32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       0.80      0.85      0.82       201\n",
      "         Pos       0.83      0.78      0.81       199\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.82      0.81      0.81       400\n",
      "weighted avg       0.82      0.81      0.81       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "df = pd.read_csv('movie-pang02.csv')\n",
    "\n",
    "X=df[\"text\"]\n",
    "y=df[\"class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "\n",
    "#one_hot vector\n",
    "vectorizer= CountVectorizer(\n",
    "    stop_words=\"english\", #Remove palavras comuns da língua inglesa (como \"the\", \"is\", \"and\")\n",
    "    min_df=20, #Filtrando as palavras que ocorreram menos de 20 vezes em todos documentos , tipo um filtro de OUTLIER\n",
    "    token_pattern=r\"(?u)\\b[a-z]{2,}\\b\", #Captura palavras com pelo menos 2 letras minúsculas\n",
    "    binary=True #Apenas indica se uma palavra está presente ou não no documento.\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)  \n",
    "\n",
    "model = MultinomialNB(alpha=1. )\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_vec)\n",
    "accuracy= accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy {accuracy}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
